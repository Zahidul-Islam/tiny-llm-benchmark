var __rest = (this && this.__rest) || function (s, e) {
    var t = {};
    for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p) && e.indexOf(p) < 0)
        t[p] = s[p];
    if (s != null && typeof Object.getOwnPropertySymbols === "function")
        for (var i = 0, p = Object.getOwnPropertySymbols(s); i < p.length; i++) {
            if (e.indexOf(p[i]) < 0 && Object.prototype.propertyIsEnumerable.call(s, p[i]))
                t[p[i]] = s[p[i]];
        }
    return t;
};
var __asyncValues = (this && this.__asyncValues) || function (o) {
    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");
    var m = o[Symbol.asyncIterator], i;
    return m ? m.call(o) : (o = typeof __values === "function" ? __values(o) : o[Symbol.iterator](), i = {}, verb("next"), verb("throw"), verb("return"), i[Symbol.asyncIterator] = function () { return this; }, i);
    function verb(n) { i[n] = o[n] && function (v) { return new Promise(function (resolve, reject) { v = o[n](v), settle(resolve, reject, v.done, v.value); }); }; }
    function settle(resolve, reject, d, v) { Promise.resolve(v).then(function(v) { resolve({ value: v, done: d }); }, reject); }
};
import { InstrumentationBase, InstrumentationNodeModuleDefinition, safeExecuteInTheMiddle, } from "@opentelemetry/instrumentation";
import { diag, context, trace, SpanKind, SpanStatusCode, } from "@opentelemetry/api";
import { VERSION } from "./version";
import { SemanticConventions, OpenInferenceSpanKind, MimeType, } from "@arizeai/openinference-semantic-conventions";
import { assertUnreachable, isString } from "./typeUtils";
import { isTracingSuppressed } from "@opentelemetry/core";
const MODULE_NAME = "openai";
/**
 * Flag to check if the openai module has been patched
 * Note: This is a fallback in case the module is made immutable (e.x. Deno, webpack, etc.)
 */
let _isOpenInferencePatched = false;
/**
 * function to check if instrumentation is enabled / disabled
 */
export function isPatched() {
    return _isOpenInferencePatched;
}
/**
 * Resolves the execution context for the current span
 * If tracing is suppressed, the span is dropped and the current context is returned
 * @param span
 */
function getExecContext(span) {
    const activeContext = context.active();
    const suppressTracing = isTracingSuppressed(activeContext);
    const execContext = suppressTracing
        ? trace.setSpan(context.active(), span)
        : activeContext;
    // Drop the span from the context
    if (suppressTracing) {
        trace.deleteSpan(activeContext);
    }
    return execContext;
}
export class OpenAIInstrumentation extends InstrumentationBase {
    constructor(config) {
        super("@arizeai/openinference-instrumentation-openai", VERSION, Object.assign({}, config));
    }
    init() {
        const module = new InstrumentationNodeModuleDefinition("openai", ["^4.0.0"], this.patch.bind(this), this.unpatch.bind(this));
        return module;
    }
    /**
     * Manually instruments the OpenAI module. This is needed when the module is not loaded via require (commonjs)
     * @param {openai} module
     */
    manuallyInstrument(module) {
        diag.debug(`Manually instrumenting ${MODULE_NAME}`);
        this.patch(module);
    }
    /**
     * Patches the OpenAI module
     */
    patch(module, moduleVersion) {
        diag.debug(`Applying patch for ${MODULE_NAME}@${moduleVersion}`);
        if ((module === null || module === void 0 ? void 0 : module.openInferencePatched) || _isOpenInferencePatched) {
            return module;
        }
        // eslint-disable-next-line @typescript-eslint/no-this-alias
        const instrumentation = this;
        // Patch create chat completions
        this._wrap(module.OpenAI.Chat.Completions.prototype, "create", 
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        (original) => {
            return function patchedCreate(...args) {
                const body = args[0];
                const { messages: _messages } = body, invocationParameters = __rest(body, ["messages"]);
                const span = instrumentation.tracer.startSpan(`OpenAI Chat Completions`, {
                    kind: SpanKind.INTERNAL,
                    attributes: Object.assign({ [SemanticConventions.OPENINFERENCE_SPAN_KIND]: OpenInferenceSpanKind.LLM, [SemanticConventions.LLM_MODEL_NAME]: body.model, [SemanticConventions.INPUT_VALUE]: JSON.stringify(body), [SemanticConventions.INPUT_MIME_TYPE]: MimeType.JSON, [SemanticConventions.LLM_INVOCATION_PARAMETERS]: JSON.stringify(invocationParameters) }, getLLMInputMessagesAttributes(body)),
                });
                const execContext = getExecContext(span);
                const execPromise = safeExecuteInTheMiddle(() => {
                    return context.with(execContext, () => {
                        return original.apply(this, args);
                    });
                }, (error) => {
                    // Push the error to the span
                    if (error) {
                        span.recordException(error);
                        span.setStatus({
                            code: SpanStatusCode.ERROR,
                            message: error.message,
                        });
                        span.end();
                    }
                });
                const wrappedPromise = execPromise.then((result) => {
                    if (isChatCompletionResponse(result)) {
                        // Record the results
                        span.setAttributes(Object.assign(Object.assign({ [SemanticConventions.OUTPUT_VALUE]: JSON.stringify(result), [SemanticConventions.OUTPUT_MIME_TYPE]: MimeType.JSON, 
                            // Override the model from the value sent by the server
                            [SemanticConventions.LLM_MODEL_NAME]: result.model }, getChatCompletionLLMOutputMessagesAttributes(result)), getUsageAttributes(result)));
                        span.setStatus({ code: SpanStatusCode.OK });
                        span.end();
                    }
                    else {
                        // This is a streaming response
                        // handle the chunks and add them to the span
                        // First split the stream via tee
                        const [leftStream, rightStream] = result.tee();
                        consumeChatCompletionStreamChunks(rightStream, span);
                        result = leftStream;
                    }
                    return result;
                });
                return context.bind(execContext, wrappedPromise);
            };
        });
        this._wrap(module.OpenAI.Completions.prototype, "create", 
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        (original) => {
            return function patchedCreate(...args) {
                const body = args[0];
                const { prompt: _prompt } = body, invocationParameters = __rest(body, ["prompt"]);
                const span = instrumentation.tracer.startSpan(`OpenAI Completions`, {
                    kind: SpanKind.INTERNAL,
                    attributes: Object.assign({ [SemanticConventions.OPENINFERENCE_SPAN_KIND]: OpenInferenceSpanKind.LLM, [SemanticConventions.LLM_MODEL_NAME]: body.model, [SemanticConventions.LLM_INVOCATION_PARAMETERS]: JSON.stringify(invocationParameters) }, getCompletionInputValueAndMimeType(body)),
                });
                const execContext = getExecContext(span);
                const execPromise = safeExecuteInTheMiddle(() => {
                    return context.with(execContext, () => {
                        return original.apply(this, args);
                    });
                }, (error) => {
                    // Push the error to the span
                    if (error) {
                        span.recordException(error);
                        span.setStatus({
                            code: SpanStatusCode.ERROR,
                            message: error.message,
                        });
                        span.end();
                    }
                });
                const wrappedPromise = execPromise.then((result) => {
                    if (isCompletionResponse(result)) {
                        // Record the results
                        span.setAttributes(Object.assign(Object.assign({ [SemanticConventions.OUTPUT_VALUE]: JSON.stringify(result), [SemanticConventions.OUTPUT_MIME_TYPE]: MimeType.JSON, 
                            // Override the model from the value sent by the server
                            [SemanticConventions.LLM_MODEL_NAME]: result.model }, getCompletionOutputValueAndMimeType(result)), getUsageAttributes(result)));
                        span.setStatus({ code: SpanStatusCode.OK });
                        span.end();
                    }
                    return result;
                });
                return context.bind(execContext, wrappedPromise);
            };
        });
        this._wrap(module.OpenAI.Embeddings.prototype, "create", 
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        (original) => {
            return function patchedEmbeddingCreate(...args) {
                const body = args[0];
                const { input } = body;
                const isStringInput = typeof input === "string";
                const span = instrumentation.tracer.startSpan(`OpenAI Embeddings`, {
                    kind: SpanKind.INTERNAL,
                    attributes: Object.assign({ [SemanticConventions.OPENINFERENCE_SPAN_KIND]: OpenInferenceSpanKind.EMBEDDING, [SemanticConventions.EMBEDDING_MODEL_NAME]: body.model, [SemanticConventions.INPUT_VALUE]: isStringInput
                            ? input
                            : JSON.stringify(input), [SemanticConventions.INPUT_MIME_TYPE]: isStringInput
                            ? MimeType.TEXT
                            : MimeType.JSON }, getEmbeddingTextAttributes(body)),
                });
                const execContext = getExecContext(span);
                const execPromise = safeExecuteInTheMiddle(() => {
                    return context.with(execContext, () => {
                        return original.apply(this, args);
                    });
                }, (error) => {
                    // Push the error to the span
                    if (error) {
                        span.recordException(error);
                        span.setStatus({
                            code: SpanStatusCode.ERROR,
                            message: error.message,
                        });
                        span.end();
                    }
                });
                const wrappedPromise = execPromise.then((result) => {
                    if (result) {
                        // Record the results
                        span.setAttributes(Object.assign({}, getEmbeddingEmbeddingsAttributes(result)));
                    }
                    span.setStatus({ code: SpanStatusCode.OK });
                    span.end();
                    return result;
                });
                return context.bind(execContext, wrappedPromise);
            };
        });
        _isOpenInferencePatched = true;
        try {
            // This can fail if the module is made immutable via the runtime or bundler
            module.openInferencePatched = true;
        }
        catch (e) {
            diag.warn(`Failed to set ${MODULE_NAME} patched flag on the module`, e);
        }
        return module;
    }
    /**
     * Un-patches the OpenAI module's chat completions API
     */
    unpatch(moduleExports, moduleVersion) {
        diag.debug(`Removing patch for ${MODULE_NAME}@${moduleVersion}`);
        this._unwrap(moduleExports.OpenAI.Chat.Completions.prototype, "create");
        this._unwrap(moduleExports.OpenAI.Completions.prototype, "create");
        this._unwrap(moduleExports.OpenAI.Embeddings.prototype, "create");
        _isOpenInferencePatched = false;
        try {
            // This can fail if the module is made immutable via the runtime or bundler
            moduleExports.openInferencePatched = false;
        }
        catch (e) {
            diag.warn(`Failed to unset ${MODULE_NAME} patched flag on the module`, e);
        }
    }
}
/**
 * type-guard that checks if the response is a chat completion response
 */
function isChatCompletionResponse(response) {
    return "choices" in response;
}
/**
 * type-guard that checks if the response is a completion response
 */
function isCompletionResponse(response) {
    return "choices" in response;
}
/**
 * type-guard that checks if completion prompt attribute is an array of strings
 */
function isPromptStringArray(prompt) {
    return (Array.isArray(prompt) && prompt.every((item) => typeof item === "string"));
}
/**
 * Converts the body of a chat completions request to LLM input messages
 */
function getLLMInputMessagesAttributes(body) {
    return body.messages.reduce((acc, message, index) => {
        const messageAttributes = getChatCompletionInputMessageAttributes(message);
        const indexPrefix = `${SemanticConventions.LLM_INPUT_MESSAGES}.${index}.`;
        // Flatten the attributes on the index prefix
        for (const [key, value] of Object.entries(messageAttributes)) {
            acc[`${indexPrefix}${key}`] = value;
        }
        return acc;
    }, {});
}
function getChatCompletionInputMessageAttributes(message) {
    const role = message.role;
    const attributes = {
        [SemanticConventions.MESSAGE_ROLE]: role,
    };
    // Add the content only if it is a string
    if (typeof message.content === "string")
        attributes[SemanticConventions.MESSAGE_CONTENT] = message.content;
    switch (role) {
        case "user":
            // There's nothing to add for the user
            break;
        case "assistant":
            if (message.tool_calls) {
                message.tool_calls.forEach((toolCall, index) => {
                    // Make sure the tool call has a function
                    if (toolCall.function) {
                        const toolCallIndexPrefix = `${SemanticConventions.MESSAGE_TOOL_CALLS}.${index}.`;
                        attributes[toolCallIndexPrefix + SemanticConventions.TOOL_CALL_FUNCTION_NAME] = toolCall.function.name;
                        attributes[toolCallIndexPrefix +
                            SemanticConventions.TOOL_CALL_FUNCTION_ARGUMENTS_JSON] = toolCall.function.arguments;
                    }
                });
            }
            break;
        case "function":
            attributes[SemanticConventions.MESSAGE_FUNCTION_CALL_NAME] = message.name;
            break;
        case "tool":
            // There's nothing to add for the tool. There is a tool_id, but there are no
            // semantic conventions for it
            break;
        case "system":
            // There's nothing to add for the system. Content is captured above
            break;
        default:
            assertUnreachable(role);
            break;
    }
    return attributes;
}
/**
 * Converts the body of a completions request to input attributes
 */
function getCompletionInputValueAndMimeType(body) {
    if (typeof body.prompt === "string") {
        return {
            [SemanticConventions.INPUT_VALUE]: body.prompt,
            [SemanticConventions.INPUT_MIME_TYPE]: MimeType.TEXT,
        };
    }
    else if (isPromptStringArray(body.prompt)) {
        const prompt = body.prompt[0]; // Only single prompts are currently supported
        if (prompt === undefined) {
            return {};
        }
        return {
            [SemanticConventions.INPUT_VALUE]: prompt,
            [SemanticConventions.INPUT_MIME_TYPE]: MimeType.TEXT,
        };
    }
    // Other cases in which the prompt is a token or array of tokens are currently unsupported
    return {};
}
/**
 * Get usage attributes
 */
function getUsageAttributes(completion) {
    if (completion.usage) {
        return {
            [SemanticConventions.LLM_TOKEN_COUNT_COMPLETION]: completion.usage.completion_tokens,
            [SemanticConventions.LLM_TOKEN_COUNT_PROMPT]: completion.usage.prompt_tokens,
            [SemanticConventions.LLM_TOKEN_COUNT_TOTAL]: completion.usage.total_tokens,
        };
    }
    return {};
}
/**
 * Converts the chat completion result to LLM output attributes
 */
function getChatCompletionLLMOutputMessagesAttributes(chatCompletion) {
    // Right now support just the first choice
    const choice = chatCompletion.choices[0];
    if (!choice) {
        return {};
    }
    return [choice.message].reduce((acc, message, index) => {
        const indexPrefix = `${SemanticConventions.LLM_OUTPUT_MESSAGES}.${index}.`;
        const messageAttributes = getChatCompletionOutputMessageAttributes(message);
        // Flatten the attributes on the index prefix
        for (const [key, value] of Object.entries(messageAttributes)) {
            acc[`${indexPrefix}${key}`] = value;
        }
        return acc;
    }, {});
}
function getChatCompletionOutputMessageAttributes(message) {
    const role = message.role;
    const attributes = {
        [SemanticConventions.MESSAGE_ROLE]: role,
    };
    if (message.content) {
        attributes[SemanticConventions.MESSAGE_CONTENT] = message.content;
    }
    if (message.tool_calls) {
        message.tool_calls.forEach((toolCall, index) => {
            const toolCallIndexPrefix = `${SemanticConventions.MESSAGE_TOOL_CALLS}.${index}.`;
            // Double check that the tool call has a function
            // NB: OpenAI only supports tool calls with functions right now but this may change
            if (toolCall.function) {
                attributes[toolCallIndexPrefix + SemanticConventions.TOOL_CALL_FUNCTION_NAME] = toolCall.function.name;
                attributes[toolCallIndexPrefix +
                    SemanticConventions.TOOL_CALL_FUNCTION_ARGUMENTS_JSON] = toolCall.function.arguments;
            }
        });
    }
    if (message.function_call) {
        attributes[SemanticConventions.MESSAGE_FUNCTION_CALL_NAME] =
            message.function_call.name;
        attributes[SemanticConventions.MESSAGE_FUNCTION_CALL_ARGUMENTS_JSON] =
            message.function_call.arguments;
    }
    return attributes;
}
/**
 * Converts the completion result to output attributes
 */
function getCompletionOutputValueAndMimeType(completion) {
    // Right now support just the first choice
    const choice = completion.choices[0];
    if (!choice) {
        return {};
    }
    return {
        [SemanticConventions.OUTPUT_VALUE]: String(choice.text),
        [SemanticConventions.OUTPUT_MIME_TYPE]: MimeType.TEXT,
    };
}
/**
 * Converts the embedding result payload to embedding attributes
 */
function getEmbeddingTextAttributes(request) {
    if (typeof request.input === "string") {
        return {
            [`${SemanticConventions.EMBEDDING_EMBEDDINGS}.0.${SemanticConventions.EMBEDDING_TEXT}`]: request.input,
        };
    }
    else if (Array.isArray(request.input) &&
        request.input.length > 0 &&
        typeof request.input[0] === "string") {
        return request.input.reduce((acc, input, index) => {
            const indexPrefix = `${SemanticConventions.EMBEDDING_EMBEDDINGS}.${index}.`;
            acc[`${indexPrefix}${SemanticConventions.EMBEDDING_TEXT}`] = input;
            return acc;
        }, {});
    }
    // Ignore other cases where input is a number or an array of numbers
    return {};
}
/**
 * Converts the embedding result payload to embedding attributes
 */
function getEmbeddingEmbeddingsAttributes(response) {
    return response.data.reduce((acc, embedding, index) => {
        const indexPrefix = `${SemanticConventions.EMBEDDING_EMBEDDINGS}.${index}.`;
        acc[`${indexPrefix}${SemanticConventions.EMBEDDING_VECTOR}`] =
            embedding.embedding;
        return acc;
    }, {});
}
/**
 * Consumes the stream chunks and adds them to the span
 */
async function consumeChatCompletionStreamChunks(stream, span) {
    var _a, e_1, _b, _c;
    let streamResponse = "";
    // Tool and function call attributes can also arrive in the stream
    // NB: the tools and function calls arrive in partial diffs
    // So the final tool and function calls need to be aggregated
    // across chunks
    const toolAndFunctionCallAttributes = {};
    try {
        // The first message is for the assistant response so we start at 1
        for (var _d = true, stream_1 = __asyncValues(stream), stream_1_1; stream_1_1 = await stream_1.next(), _a = stream_1_1.done, !_a; _d = true) {
            _c = stream_1_1.value;
            _d = false;
            const chunk = _c;
            if (chunk.choices.length <= 0) {
                continue;
            }
            const choice = chunk.choices[0];
            if (choice.delta.content) {
                streamResponse += choice.delta.content;
            }
            // Accumulate the tool and function call attributes
            const toolAndFunctionCallAttributesDiff = getToolAndFunctionCallAttributesFromStreamChunk(chunk);
            for (const [key, value] of Object.entries(toolAndFunctionCallAttributesDiff)) {
                if (isString(toolAndFunctionCallAttributes[key]) && isString(value)) {
                    toolAndFunctionCallAttributes[key] += value;
                }
                else if (isString(value)) {
                    toolAndFunctionCallAttributes[key] = value;
                }
            }
        }
    }
    catch (e_1_1) { e_1 = { error: e_1_1 }; }
    finally {
        try {
            if (!_d && !_a && (_b = stream_1.return)) await _b.call(stream_1);
        }
        finally { if (e_1) throw e_1.error; }
    }
    const messageIndexPrefix = `${SemanticConventions.LLM_OUTPUT_MESSAGES}.0.`;
    // Append the attributes to the span as a message
    const attributes = {
        [SemanticConventions.OUTPUT_VALUE]: streamResponse,
        [SemanticConventions.OUTPUT_MIME_TYPE]: MimeType.TEXT,
        [`${messageIndexPrefix}${SemanticConventions.MESSAGE_CONTENT}`]: streamResponse,
        [`${messageIndexPrefix}${SemanticConventions.MESSAGE_ROLE}`]: "assistant",
    };
    // Add the tool and function call attributes
    for (const [key, value] of Object.entries(toolAndFunctionCallAttributes)) {
        attributes[`${messageIndexPrefix}${key}`] = value;
    }
    span.setAttributes(attributes);
    span.end();
}
/**
 * Extracts the semantic attributes from the stream chunk for tool_calls and function_calls
 */
function getToolAndFunctionCallAttributesFromStreamChunk(chunk) {
    if (chunk.choices.length <= 0) {
        return {};
    }
    const choice = chunk.choices[0];
    const attributes = {};
    if (choice.delta.tool_calls) {
        choice.delta.tool_calls.forEach((toolCall, index) => {
            const toolCallIndexPrefix = `${SemanticConventions.MESSAGE_TOOL_CALLS}.${index}.`;
            // Double check that the tool call has a function
            // NB: OpenAI only supports tool calls with functions right now but this may change
            if (toolCall.function) {
                attributes[toolCallIndexPrefix + SemanticConventions.TOOL_CALL_FUNCTION_NAME] = toolCall.function.name;
                attributes[toolCallIndexPrefix +
                    SemanticConventions.TOOL_CALL_FUNCTION_ARGUMENTS_JSON] = toolCall.function.arguments;
            }
        });
    }
    if (choice.delta.function_call) {
        attributes[SemanticConventions.MESSAGE_FUNCTION_CALL_NAME] =
            choice.delta.function_call.name;
        attributes[SemanticConventions.MESSAGE_FUNCTION_CALL_ARGUMENTS_JSON] =
            choice.delta.function_call.arguments;
    }
    return attributes;
}
//# sourceMappingURL=instrumentation.js.map
"use strict";
var __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    var desc = Object.getOwnPropertyDescriptor(m, k);
    if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
      desc = { enumerable: true, get: function() { return m[k]; } };
    }
    Object.defineProperty(o, k2, desc);
}) : (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    o[k2] = m[k];
}));
var __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {
    Object.defineProperty(o, "default", { enumerable: true, value: v });
}) : function(o, v) {
    o["default"] = v;
});
var __importStar = (this && this.__importStar) || function (mod) {
    if (mod && mod.__esModule) return mod;
    var result = {};
    if (mod != null) for (var k in mod) if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);
    __setModuleDefault(result, mod);
    return result;
};
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __await = (this && this.__await) || function (v) { return this instanceof __await ? (this.v = v, this) : new __await(v); }
var __asyncGenerator = (this && this.__asyncGenerator) || function (thisArg, _arguments, generator) {
    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");
    var g = generator.apply(thisArg, _arguments || []), i, q = [];
    return i = {}, verb("next"), verb("throw"), verb("return", awaitReturn), i[Symbol.asyncIterator] = function () { return this; }, i;
    function awaitReturn(f) { return function (v) { return Promise.resolve(v).then(f, reject); }; }
    function verb(n, f) { if (g[n]) { i[n] = function (v) { return new Promise(function (a, b) { q.push([n, v, a, b]) > 1 || resume(n, v); }); }; if (f) i[n] = f(i[n]); } }
    function resume(n, v) { try { step(g[n](v)); } catch (e) { settle(q[0][3], e); } }
    function step(r) { r.value instanceof __await ? Promise.resolve(r.value.v).then(fulfill, reject) : settle(q[0][2], r); }
    function fulfill(value) { resume("next", value); }
    function reject(value) { resume("throw", value); }
    function settle(f, v) { if (f(v), q.shift(), q.length) resume(q[0][0], q[0][1]); }
};
var __asyncValues = (this && this.__asyncValues) || function (o) {
    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");
    var m = o[Symbol.asyncIterator], i;
    return m ? m.call(o) : (o = typeof __values === "function" ? __values(o) : o[Symbol.iterator](), i = {}, verb("next"), verb("throw"), verb("return"), i[Symbol.asyncIterator] = function () { return this; }, i);
    function verb(n) { i[n] = o[n] && function (v) { return new Promise(function (resolve, reject) { v = o[n](v), settle(resolve, reject, v.done, v.value); }); }; }
    function settle(resolve, reject, d, v) { Promise.resolve(v).then(function(v) { resolve({ value: v, done: d }); }, reject); }
};
Object.defineProperty(exports, "__esModule", { value: true });
const src_1 = require("../src");
const sdk_trace_base_1 = require("@opentelemetry/sdk-trace-base");
const sdk_trace_node_1 = require("@opentelemetry/sdk-trace-node");
const core_1 = require("@opentelemetry/core");
const api_1 = require("@opentelemetry/api");
const tracerProvider = new sdk_trace_node_1.NodeTracerProvider();
tracerProvider.register();
const instrumentation = new src_1.OpenAIInstrumentation();
instrumentation.disable();
const OpenAI = __importStar(require("openai"));
const streaming_1 = require("openai/streaming");
// Function tools
function getCurrentLocation() {
    return __awaiter(this, void 0, void 0, function* () {
        return "Boston"; // Simulate lookup
    });
}
function getWeather(_args) {
    return __awaiter(this, void 0, void 0, function* () {
        return { temperature: 52, precipitation: "rainy" };
    });
}
describe("OpenAIInstrumentation", () => {
    let openai;
    const memoryExporter = new sdk_trace_base_1.InMemorySpanExporter();
    instrumentation.setTracerProvider(tracerProvider);
    tracerProvider.addSpanProcessor(new sdk_trace_base_1.SimpleSpanProcessor(memoryExporter));
    // @ts-expect-error the moduleExports property is private. This is needed to make the test work with auto-mocking
    instrumentation._modules[0].moduleExports = OpenAI;
    beforeAll(() => {
        instrumentation.enable();
        openai = new OpenAI.OpenAI({
            apiKey: "fake-api-key",
        });
    });
    afterAll(() => {
        instrumentation.disable();
    });
    beforeEach(() => {
        memoryExporter.reset();
    });
    afterEach(() => {
        jest.clearAllMocks();
    });
    it("is patched", () => {
        expect(OpenAI.openInferencePatched).toBe(true);
        expect((0, src_1.isPatched)()).toBe(true);
    });
    it("sets a patched flag correctly to track whether or not openai is instrumented", () => {
        instrumentation.disable();
        expect((0, src_1.isPatched)()).toBe(false);
        instrumentation.enable();
        expect((0, src_1.isPatched)()).toBe(true);
    });
    it("creates a span for chat completions", () => __awaiter(void 0, void 0, void 0, function* () {
        const response = {
            id: "chatcmpl-8adq9JloOzNZ9TyuzrKyLpGXexh6p",
            object: "chat.completion",
            created: 1703743645,
            model: "gpt-3.5-turbo-0613",
            choices: [
                {
                    index: 0,
                    message: {
                        role: "assistant",
                        content: "This is a test.",
                    },
                    logprobs: null,
                    finish_reason: "stop",
                },
            ],
            usage: {
                prompt_tokens: 12,
                completion_tokens: 5,
                total_tokens: 17,
            },
        };
        // Mock out the chat completions endpoint
        jest.spyOn(openai, "post").mockImplementation(
        // @ts-expect-error the response type is not correct - this is just for testing
        () => __awaiter(void 0, void 0, void 0, function* () {
            return response;
        }));
        yield openai.chat.completions.create({
            messages: [{ role: "user", content: "Say this is a test" }],
            model: "gpt-3.5-turbo",
        });
        const spans = memoryExporter.getFinishedSpans();
        expect(spans.length).toBe(1);
        const span = spans[0];
        expect(span.name).toBe("OpenAI Chat Completions");
        expect(span.attributes).toMatchInlineSnapshot(`
      {
        "input.mime_type": "application/json",
        "input.value": "{"messages":[{"role":"user","content":"Say this is a test"}],"model":"gpt-3.5-turbo"}",
        "llm.input_messages.0.message.content": "Say this is a test",
        "llm.input_messages.0.message.role": "user",
        "llm.invocation_parameters": "{"model":"gpt-3.5-turbo"}",
        "llm.model_name": "gpt-3.5-turbo-0613",
        "llm.output_messages.0.message.content": "This is a test.",
        "llm.output_messages.0.message.role": "assistant",
        "llm.token_count.completion": 5,
        "llm.token_count.prompt": 12,
        "llm.token_count.total": 17,
        "openinference.span.kind": "llm",
        "output.mime_type": "application/json",
        "output.value": "{"id":"chatcmpl-8adq9JloOzNZ9TyuzrKyLpGXexh6p","object":"chat.completion","created":1703743645,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"This is a test."},"logprobs":null,"finish_reason":"stop"}],"usage":{"prompt_tokens":12,"completion_tokens":5,"total_tokens":17}}",
      }
    `);
    }));
    it("creates a span for completions", () => __awaiter(void 0, void 0, void 0, function* () {
        const response = {
            id: "cmpl-8fZu1H3VijJUWev9asnxaYyQvJTC9",
            object: "text_completion",
            created: 1704920149,
            model: "gpt-3.5-turbo-instruct",
            choices: [
                {
                    text: "This is a test",
                    index: 0,
                    logprobs: null,
                    finish_reason: "stop",
                },
            ],
            usage: { prompt_tokens: 12, completion_tokens: 5, total_tokens: 17 },
        };
        // Mock out the completions endpoint
        jest.spyOn(openai, "post").mockImplementation(
        // @ts-expect-error the response type is not correct - this is just for testing
        () => __awaiter(void 0, void 0, void 0, function* () {
            return response;
        }));
        yield openai.completions.create({
            prompt: "Say this is a test",
            model: "gpt-3.5-turbo-instruct",
        });
        const spans = memoryExporter.getFinishedSpans();
        expect(spans.length).toBe(1);
        const span = spans[0];
        expect(span.name).toBe("OpenAI Completions");
        expect(span.attributes).toMatchInlineSnapshot(`
      {
        "input.mime_type": "text/plain",
        "input.value": "Say this is a test",
        "llm.invocation_parameters": "{"model":"gpt-3.5-turbo-instruct"}",
        "llm.model_name": "gpt-3.5-turbo-instruct",
        "llm.token_count.completion": 5,
        "llm.token_count.prompt": 12,
        "llm.token_count.total": 17,
        "openinference.span.kind": "llm",
        "output.mime_type": "text/plain",
        "output.value": "This is a test",
      }
    `);
    }));
    it("creates a span for embedding create", () => __awaiter(void 0, void 0, void 0, function* () {
        const response = {
            object: "list",
            data: [{ object: "embedding", index: 0, embedding: [1, 2, 3] }],
        };
        // Mock out the embedding create endpoint
        jest.spyOn(openai, "post").mockImplementation(
        // @ts-expect-error the response type is not correct - this is just for testing
        () => __awaiter(void 0, void 0, void 0, function* () {
            return response;
        }));
        yield openai.embeddings.create({
            input: "A happy moment",
            model: "text-embedding-ada-002",
        });
        const spans = memoryExporter.getFinishedSpans();
        expect(spans.length).toBe(1);
        const span = spans[0];
        expect(span.name).toBe("OpenAI Embeddings");
        expect(span.attributes).toMatchInlineSnapshot(`
      {
        "embedding.embeddings.0.embedding.text": "A happy moment",
        "embedding.embeddings.0.embedding.vector": [
          1,
          2,
          3,
        ],
        "embedding.model_name": "text-embedding-ada-002",
        "input.mime_type": "text/plain",
        "input.value": "A happy moment",
        "openinference.span.kind": "embedding",
      }
    `);
    }));
    it("can handle streaming responses", () => __awaiter(void 0, void 0, void 0, function* () {
        var _a, e_1, _b, _c;
        // Mock out the post endpoint to return a stream
        jest.spyOn(openai, "post").mockImplementation(
        // @ts-expect-error the response type is not correct - this is just for testing
        () => __awaiter(void 0, void 0, void 0, function* () {
            const iterator = () => (function () {
                return __asyncGenerator(this, arguments, function* () {
                    yield yield __await({ choices: [{ delta: { content: "This is " } }] });
                    yield yield __await({ choices: [{ delta: { content: "a test." } }] });
                    yield yield __await({ choices: [{ delta: { finish_reason: "stop" } }] });
                });
            })();
            const controller = new AbortController();
            return new streaming_1.Stream(iterator, controller);
        }));
        const stream = yield openai.chat.completions.create({
            messages: [{ role: "user", content: "Say this is a test" }],
            model: "gpt-3.5-turbo",
            stream: true,
        });
        let response = "";
        try {
            for (var _d = true, stream_1 = __asyncValues(stream), stream_1_1; stream_1_1 = yield stream_1.next(), _a = stream_1_1.done, !_a; _d = true) {
                _c = stream_1_1.value;
                _d = false;
                const chunk = _c;
                if (chunk.choices[0].delta.content)
                    response += chunk.choices[0].delta.content;
            }
        }
        catch (e_1_1) { e_1 = { error: e_1_1 }; }
        finally {
            try {
                if (!_d && !_a && (_b = stream_1.return)) yield _b.call(stream_1);
            }
            finally { if (e_1) throw e_1.error; }
        }
        expect(response).toBe("This is a test.");
        const spans = memoryExporter.getFinishedSpans();
        expect(spans.length).toBe(1);
        const span = spans[0];
        expect(span.name).toBe("OpenAI Chat Completions");
        expect(span.attributes).toMatchInlineSnapshot(`
      {
        "input.mime_type": "application/json",
        "input.value": "{"messages":[{"role":"user","content":"Say this is a test"}],"model":"gpt-3.5-turbo","stream":true}",
        "llm.input_messages.0.message.content": "Say this is a test",
        "llm.input_messages.0.message.role": "user",
        "llm.invocation_parameters": "{"model":"gpt-3.5-turbo","stream":true}",
        "llm.model_name": "gpt-3.5-turbo",
        "llm.output_messages.0.message.content": "This is a test.",
        "llm.output_messages.0.message.role": "assistant",
        "openinference.span.kind": "llm",
        "output.mime_type": "text/plain",
        "output.value": "This is a test.",
      }
    `);
    }));
    it("should capture tool calls", () => __awaiter(void 0, void 0, void 0, function* () {
        // Mock out the embedding create endpoint
        const response1 = {
            id: "chatcmpl-8hhqZDFTRD0vzExhqWnMLE7viVl7E",
            object: "chat.completion",
            created: 1705427343,
            model: "gpt-3.5-turbo-0613",
            choices: [
                {
                    index: 0,
                    message: {
                        role: "assistant",
                        content: null,
                        tool_calls: [
                            {
                                id: "call_5ERYvu4iTGSvDlcDQjDP3g3J",
                                type: "function",
                                function: { name: "getCurrentLocation", arguments: "{}" },
                            },
                        ],
                    },
                    logprobs: null,
                    finish_reason: "tool_calls",
                },
            ],
            usage: { prompt_tokens: 70, completion_tokens: 7, total_tokens: 77 },
            system_fingerprint: null,
        };
        const response2 = {
            id: "chatcmpl-8hhsP9eAplUFYB3mHUJxBkq7IwnjZ",
            object: "chat.completion",
            created: 1705427457,
            model: "gpt-3.5-turbo-0613",
            choices: [
                {
                    index: 0,
                    message: {
                        role: "assistant",
                        content: null,
                        tool_calls: [
                            {
                                id: "call_0LCdYLkdRUt3rV3dawoIFHBf",
                                type: "function",
                                function: {
                                    name: "getWeather",
                                    arguments: '{\n  "location": "Boston"\n}',
                                },
                            },
                        ],
                    },
                    logprobs: null,
                    finish_reason: "tool_calls",
                },
            ],
            usage: { prompt_tokens: 86, completion_tokens: 15, total_tokens: 101 },
            system_fingerprint: null,
        };
        const response3 = {
            id: "chatcmpl-8hhtfzSD33tsG7XJiBg4F9MqnXKDp",
            object: "chat.completion",
            created: 1705427535,
            model: "gpt-3.5-turbo-0613",
            choices: [
                {
                    index: 0,
                    message: {
                        role: "assistant",
                        content: "The weather in Boston this week is expected to be rainy with a temperature of 52 degrees.",
                    },
                    logprobs: null,
                    finish_reason: "stop",
                },
            ],
            usage: { prompt_tokens: 121, completion_tokens: 20, total_tokens: 141 },
            system_fingerprint: null,
        };
        jest
            .spyOn(openai, "post")
            .mockImplementationOnce(
        // @ts-expect-error the response type is not correct - this is just for testing
        () => __awaiter(void 0, void 0, void 0, function* () {
            return response1;
        }))
            .mockImplementationOnce(
        // @ts-expect-error the response type is not correct - this is just for testing
        () => __awaiter(void 0, void 0, void 0, function* () {
            return response2;
        }))
            .mockImplementationOnce(
        // @ts-expect-error the response type is not correct - this is just for testing
        () => __awaiter(void 0, void 0, void 0, function* () {
            return response3;
        }));
        const messages = [];
        const runner = openai.beta.chat.completions
            .runTools({
            model: "gpt-3.5-turbo",
            messages: [{ role: "user", content: "How is the weather this week?" }],
            tools: [
                {
                    type: "function",
                    function: {
                        function: getCurrentLocation,
                        parameters: { type: "object", properties: {} },
                        description: "Get the current location of the user.",
                    },
                },
                {
                    type: "function",
                    function: {
                        function: getWeather,
                        parse: JSON.parse, // or use a validation library like zod for typesafe parsing.
                        description: "Get the weather for a location.",
                        parameters: {
                            type: "object",
                            properties: {
                                location: { type: "string" },
                            },
                        },
                    },
                },
            ],
        })
            .on("message", (message) => messages.push(message));
        const _finalContent = yield runner.finalContent();
        const spans = memoryExporter.getFinishedSpans();
        expect(spans.length).toBe(3);
        const [span1, span2, span3] = spans;
        expect(span1.name).toBe("OpenAI Chat Completions");
        expect(span1.attributes).toMatchInlineSnapshot(`
      {
        "input.mime_type": "application/json",
        "input.value": "{"model":"gpt-3.5-turbo","messages":[{"role":"user","content":"How is the weather this week?"}],"tools":[{"type":"function","function":{"name":"getCurrentLocation","parameters":{"type":"object","properties":{}},"description":"Get the current location of the user."}},{"type":"function","function":{"name":"getWeather","parameters":{"type":"object","properties":{"location":{"type":"string"}}},"description":"Get the weather for a location."}}],"tool_choice":"auto","stream":false}",
        "llm.input_messages.0.message.content": "How is the weather this week?",
        "llm.input_messages.0.message.role": "user",
        "llm.invocation_parameters": "{"model":"gpt-3.5-turbo","tools":[{"type":"function","function":{"name":"getCurrentLocation","parameters":{"type":"object","properties":{}},"description":"Get the current location of the user."}},{"type":"function","function":{"name":"getWeather","parameters":{"type":"object","properties":{"location":{"type":"string"}}},"description":"Get the weather for a location."}}],"tool_choice":"auto","stream":false}",
        "llm.model_name": "gpt-3.5-turbo-0613",
        "llm.output_messages.0.message.role": "assistant",
        "llm.output_messages.0.message.tool_calls.0.tool_call.function.arguments": "{}",
        "llm.output_messages.0.message.tool_calls.0.tool_call.function.name": "getCurrentLocation",
        "llm.token_count.completion": 7,
        "llm.token_count.prompt": 70,
        "llm.token_count.total": 77,
        "openinference.span.kind": "llm",
        "output.mime_type": "application/json",
        "output.value": "{"id":"chatcmpl-8hhqZDFTRD0vzExhqWnMLE7viVl7E","object":"chat.completion","created":1705427343,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":null,"tool_calls":[{"id":"call_5ERYvu4iTGSvDlcDQjDP3g3J","type":"function","function":{"name":"getCurrentLocation","arguments":"{}"}}]},"logprobs":null,"finish_reason":"tool_calls"}],"usage":{"prompt_tokens":70,"completion_tokens":7,"total_tokens":77},"system_fingerprint":null}",
      }
    `);
        expect(span2.name).toBe("OpenAI Chat Completions");
        expect(span2.attributes).toMatchInlineSnapshot(`
      {
        "input.mime_type": "application/json",
        "input.value": "{"model":"gpt-3.5-turbo","messages":[{"role":"user","content":"How is the weather this week?"},{"role":"assistant","content":null,"tool_calls":[{"id":"call_5ERYvu4iTGSvDlcDQjDP3g3J","type":"function","function":{"name":"getCurrentLocation","arguments":"{}"}}]},{"role":"tool","tool_call_id":"call_5ERYvu4iTGSvDlcDQjDP3g3J","content":"Boston"}],"tools":[{"type":"function","function":{"name":"getCurrentLocation","parameters":{"type":"object","properties":{}},"description":"Get the current location of the user."}},{"type":"function","function":{"name":"getWeather","parameters":{"type":"object","properties":{"location":{"type":"string"}}},"description":"Get the weather for a location."}}],"tool_choice":"auto","stream":false}",
        "llm.input_messages.0.message.content": "How is the weather this week?",
        "llm.input_messages.0.message.role": "user",
        "llm.input_messages.1.message.role": "assistant",
        "llm.input_messages.1.message.tool_calls.0.tool_call.function.arguments": "{}",
        "llm.input_messages.1.message.tool_calls.0.tool_call.function.name": "getCurrentLocation",
        "llm.input_messages.2.message.content": "Boston",
        "llm.input_messages.2.message.role": "tool",
        "llm.invocation_parameters": "{"model":"gpt-3.5-turbo","tools":[{"type":"function","function":{"name":"getCurrentLocation","parameters":{"type":"object","properties":{}},"description":"Get the current location of the user."}},{"type":"function","function":{"name":"getWeather","parameters":{"type":"object","properties":{"location":{"type":"string"}}},"description":"Get the weather for a location."}}],"tool_choice":"auto","stream":false}",
        "llm.model_name": "gpt-3.5-turbo-0613",
        "llm.output_messages.0.message.role": "assistant",
        "llm.output_messages.0.message.tool_calls.0.tool_call.function.arguments": "{
        "location": "Boston"
      }",
        "llm.output_messages.0.message.tool_calls.0.tool_call.function.name": "getWeather",
        "llm.token_count.completion": 15,
        "llm.token_count.prompt": 86,
        "llm.token_count.total": 101,
        "openinference.span.kind": "llm",
        "output.mime_type": "application/json",
        "output.value": "{"id":"chatcmpl-8hhsP9eAplUFYB3mHUJxBkq7IwnjZ","object":"chat.completion","created":1705427457,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":null,"tool_calls":[{"id":"call_0LCdYLkdRUt3rV3dawoIFHBf","type":"function","function":{"name":"getWeather","arguments":"{\\n  \\"location\\": \\"Boston\\"\\n}"}}]},"logprobs":null,"finish_reason":"tool_calls"}],"usage":{"prompt_tokens":86,"completion_tokens":15,"total_tokens":101},"system_fingerprint":null}",
      }
    `);
        expect(span3.name).toBe("OpenAI Chat Completions");
        expect(span3.attributes).toMatchInlineSnapshot(`
      {
        "input.mime_type": "application/json",
        "input.value": "{"model":"gpt-3.5-turbo","messages":[{"role":"user","content":"How is the weather this week?"},{"role":"assistant","content":null,"tool_calls":[{"id":"call_5ERYvu4iTGSvDlcDQjDP3g3J","type":"function","function":{"name":"getCurrentLocation","arguments":"{}"}}]},{"role":"tool","tool_call_id":"call_5ERYvu4iTGSvDlcDQjDP3g3J","content":"Boston"},{"role":"assistant","content":null,"tool_calls":[{"id":"call_0LCdYLkdRUt3rV3dawoIFHBf","type":"function","function":{"name":"getWeather","arguments":"{\\n  \\"location\\": \\"Boston\\"\\n}"}}]},{"role":"tool","tool_call_id":"call_0LCdYLkdRUt3rV3dawoIFHBf","content":"{\\"temperature\\":52,\\"precipitation\\":\\"rainy\\"}"}],"tools":[{"type":"function","function":{"name":"getCurrentLocation","parameters":{"type":"object","properties":{}},"description":"Get the current location of the user."}},{"type":"function","function":{"name":"getWeather","parameters":{"type":"object","properties":{"location":{"type":"string"}}},"description":"Get the weather for a location."}}],"tool_choice":"auto","stream":false}",
        "llm.input_messages.0.message.content": "How is the weather this week?",
        "llm.input_messages.0.message.role": "user",
        "llm.input_messages.1.message.role": "assistant",
        "llm.input_messages.1.message.tool_calls.0.tool_call.function.arguments": "{}",
        "llm.input_messages.1.message.tool_calls.0.tool_call.function.name": "getCurrentLocation",
        "llm.input_messages.2.message.content": "Boston",
        "llm.input_messages.2.message.role": "tool",
        "llm.input_messages.3.message.role": "assistant",
        "llm.input_messages.3.message.tool_calls.0.tool_call.function.arguments": "{
        "location": "Boston"
      }",
        "llm.input_messages.3.message.tool_calls.0.tool_call.function.name": "getWeather",
        "llm.input_messages.4.message.content": "{"temperature":52,"precipitation":"rainy"}",
        "llm.input_messages.4.message.role": "tool",
        "llm.invocation_parameters": "{"model":"gpt-3.5-turbo","tools":[{"type":"function","function":{"name":"getCurrentLocation","parameters":{"type":"object","properties":{}},"description":"Get the current location of the user."}},{"type":"function","function":{"name":"getWeather","parameters":{"type":"object","properties":{"location":{"type":"string"}}},"description":"Get the weather for a location."}}],"tool_choice":"auto","stream":false}",
        "llm.model_name": "gpt-3.5-turbo-0613",
        "llm.output_messages.0.message.content": "The weather in Boston this week is expected to be rainy with a temperature of 52 degrees.",
        "llm.output_messages.0.message.role": "assistant",
        "llm.token_count.completion": 20,
        "llm.token_count.prompt": 121,
        "llm.token_count.total": 141,
        "openinference.span.kind": "llm",
        "output.mime_type": "application/json",
        "output.value": "{"id":"chatcmpl-8hhtfzSD33tsG7XJiBg4F9MqnXKDp","object":"chat.completion","created":1705427535,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The weather in Boston this week is expected to be rainy with a temperature of 52 degrees."},"logprobs":null,"finish_reason":"stop"}],"usage":{"prompt_tokens":121,"completion_tokens":20,"total_tokens":141},"system_fingerprint":null}",
      }
    `);
    }));
    it("should capture tool calls with streaming", () => __awaiter(void 0, void 0, void 0, function* () {
        var _e, e_2, _f, _g;
        jest.spyOn(openai, "post").mockImplementation(
        // @ts-expect-error the response type is not correct - this is just for testing
        () => __awaiter(void 0, void 0, void 0, function* () {
            const iterator = () => (function () {
                return __asyncGenerator(this, arguments, function* () {
                    yield yield __await({
                        id: "chatcmpl-8iA39kCtuVHIVDr9AnBdJZjgSjNWL",
                        object: "chat.completion.chunk",
                        created: 1705535755,
                        model: "gpt-3.5-turbo-0613",
                        system_fingerprint: null,
                        choices: [
                            {
                                index: 0,
                                delta: {
                                    role: "assistant",
                                    content: null,
                                    tool_calls: [
                                        {
                                            index: 0,
                                            id: "call_PGkcUg2u6vYrCpTn0e9ofykY",
                                            type: "function",
                                            function: { name: "getWeather", arguments: "" },
                                        },
                                    ],
                                },
                                logprobs: null,
                                finish_reason: null,
                            },
                        ],
                    });
                    yield yield __await({
                        id: "chatcmpl-8iA39kCtuVHIVDr9AnBdJZjgSjNWL",
                        object: "chat.completion.chunk",
                        created: 1705535755,
                        model: "gpt-3.5-turbo-0613",
                        system_fingerprint: null,
                        choices: [
                            {
                                index: 0,
                                delta: {
                                    tool_calls: [{ index: 0, function: { arguments: "{}" } }],
                                },
                                logprobs: null,
                                finish_reason: null,
                            },
                        ],
                    });
                    yield yield __await({
                        id: "chatcmpl-8iA39kCtuVHIVDr9AnBdJZjgSjNWL",
                        object: "chat.completion.chunk",
                        created: 1705535755,
                        model: "gpt-3.5-turbo-0613",
                        system_fingerprint: null,
                        choices: [
                            {
                                index: 0,
                                delta: {},
                                logprobs: null,
                                finish_reason: "tool_calls",
                            },
                        ],
                    });
                });
            })();
            const controller = new AbortController();
            return new streaming_1.Stream(iterator, controller);
        }));
        const stream = yield openai.chat.completions.create({
            messages: [{ role: "user", content: "What's the weather today?" }],
            model: "gpt-3.5-turbo",
            tools: [
                {
                    type: "function",
                    function: {
                        name: "getCurrentLocation",
                        parameters: { type: "object", properties: {} },
                        description: "Get the current location of the user.",
                    },
                },
                {
                    type: "function",
                    function: {
                        name: "getWeather",
                        description: "Get the weather for a location.",
                        parameters: {
                            type: "object",
                            properties: {
                                location: { type: "string" },
                            },
                        },
                    },
                },
            ],
            stream: true,
        });
        let response = "";
        try {
            for (var _h = true, stream_2 = __asyncValues(stream), stream_2_1; stream_2_1 = yield stream_2.next(), _e = stream_2_1.done, !_e; _h = true) {
                _g = stream_2_1.value;
                _h = false;
                const chunk = _g;
                if (chunk.choices[0].delta.content)
                    response += chunk.choices[0].delta.content;
            }
        }
        catch (e_2_1) { e_2 = { error: e_2_1 }; }
        finally {
            try {
                if (!_h && !_e && (_f = stream_2.return)) yield _f.call(stream_2);
            }
            finally { if (e_2) throw e_2.error; }
        }
        // When a tool is called, the content is empty
        expect(response).toBe("");
        const spans = memoryExporter.getFinishedSpans();
        expect(spans.length).toBe(1);
        const span = spans[0];
        expect(span.name).toBe("OpenAI Chat Completions");
        expect(span.attributes).toMatchInlineSnapshot(`
{
  "input.mime_type": "application/json",
  "input.value": "{"messages":[{"role":"user","content":"What's the weather today?"}],"model":"gpt-3.5-turbo","tools":[{"type":"function","function":{"name":"getCurrentLocation","parameters":{"type":"object","properties":{}},"description":"Get the current location of the user."}},{"type":"function","function":{"name":"getWeather","description":"Get the weather for a location.","parameters":{"type":"object","properties":{"location":{"type":"string"}}}}}],"stream":true}",
  "llm.input_messages.0.message.content": "What's the weather today?",
  "llm.input_messages.0.message.role": "user",
  "llm.invocation_parameters": "{"model":"gpt-3.5-turbo","tools":[{"type":"function","function":{"name":"getCurrentLocation","parameters":{"type":"object","properties":{}},"description":"Get the current location of the user."}},{"type":"function","function":{"name":"getWeather","description":"Get the weather for a location.","parameters":{"type":"object","properties":{"location":{"type":"string"}}}}}],"stream":true}",
  "llm.model_name": "gpt-3.5-turbo",
  "llm.output_messages.0.message.content": "",
  "llm.output_messages.0.message.role": "assistant",
  "llm.output_messages.0.message.tool_calls.0.tool_call.function.arguments": "{}",
  "llm.output_messages.0.message.tool_calls.0.tool_call.function.name": "getWeather",
  "openinference.span.kind": "llm",
  "output.mime_type": "text/plain",
  "output.value": "",
}
`);
    }));
    it("should capture a function call with streaming", () => __awaiter(void 0, void 0, void 0, function* () {
        var _j, e_3, _k, _l;
        jest.spyOn(openai, "post").mockImplementation(
        // @ts-expect-error the response type is not correct - this is just for testing
        () => __awaiter(void 0, void 0, void 0, function* () {
            const iterator = () => (function () {
                return __asyncGenerator(this, arguments, function* () {
                    yield yield __await({
                        id: "chatcmpl-8iA39kCtuVHIVDr9AnBdJZjgSjNWL",
                        object: "chat.completion.chunk",
                        created: 1705535755,
                        model: "gpt-3.5-turbo-0613",
                        system_fingerprint: null,
                        choices: [
                            {
                                index: 0,
                                delta: {
                                    role: "assistant",
                                    content: null,
                                    function_call: { name: "getWeather", arguments: "" },
                                },
                                logprobs: null,
                                finish_reason: null,
                            },
                        ],
                    });
                    yield yield __await({
                        id: "chatcmpl-8iA39kCtuVHIVDr9AnBdJZjgSjNWL",
                        object: "chat.completion.chunk",
                        created: 1705535755,
                        model: "gpt-3.5-turbo-0613",
                        system_fingerprint: null,
                        choices: [
                            {
                                index: 0,
                                delta: { function_call: { arguments: "{}" } },
                                logprobs: null,
                                finish_reason: null,
                            },
                        ],
                    });
                    yield yield __await({
                        id: "chatcmpl-8iA39kCtuVHIVDr9AnBdJZjgSjNWL",
                        object: "chat.completion.chunk",
                        created: 1705535755,
                        model: "gpt-3.5-turbo-0613",
                        system_fingerprint: null,
                        choices: [
                            {
                                index: 0,
                                delta: {},
                                logprobs: null,
                                finish_reason: "function_call",
                            },
                        ],
                    });
                });
            })();
            const controller = new AbortController();
            return new streaming_1.Stream(iterator, controller);
        }));
        const stream = yield openai.chat.completions.create({
            messages: [{ role: "user", content: "What's the weather today?" }],
            model: "gpt-3.5-turbo",
            functions: [
                {
                    name: "getWeather",
                    description: "Get the weather for a location.",
                    parameters: {
                        type: "object",
                        properties: {
                            location: { type: "string" },
                        },
                    },
                },
                {
                    name: "getCurrentLocation",
                    description: "Get the current location of the user.",
                    parameters: { type: "object", properties: {} },
                },
            ],
            stream: true,
        });
        let response = "";
        try {
            for (var _m = true, stream_3 = __asyncValues(stream), stream_3_1; stream_3_1 = yield stream_3.next(), _j = stream_3_1.done, !_j; _m = true) {
                _l = stream_3_1.value;
                _m = false;
                const chunk = _l;
                if (chunk.choices[0].delta.content)
                    response += chunk.choices[0].delta.content;
            }
        }
        catch (e_3_1) { e_3 = { error: e_3_1 }; }
        finally {
            try {
                if (!_m && !_j && (_k = stream_3.return)) yield _k.call(stream_3);
            }
            finally { if (e_3) throw e_3.error; }
        }
        // When a tool is called, the content is empty
        expect(response).toBe("");
        const spans = memoryExporter.getFinishedSpans();
        expect(spans.length).toBe(1);
        const span = spans[0];
        expect(span.name).toBe("OpenAI Chat Completions");
        expect(span.attributes).toMatchInlineSnapshot(`
      {
        "input.mime_type": "application/json",
        "input.value": "{"messages":[{"role":"user","content":"What's the weather today?"}],"model":"gpt-3.5-turbo","functions":[{"name":"getWeather","description":"Get the weather for a location.","parameters":{"type":"object","properties":{"location":{"type":"string"}}}},{"name":"getCurrentLocation","description":"Get the current location of the user.","parameters":{"type":"object","properties":{}}}],"stream":true}",
        "llm.input_messages.0.message.content": "What's the weather today?",
        "llm.input_messages.0.message.role": "user",
        "llm.invocation_parameters": "{"model":"gpt-3.5-turbo","functions":[{"name":"getWeather","description":"Get the weather for a location.","parameters":{"type":"object","properties":{"location":{"type":"string"}}}},{"name":"getCurrentLocation","description":"Get the current location of the user.","parameters":{"type":"object","properties":{}}}],"stream":true}",
        "llm.model_name": "gpt-3.5-turbo",
        "llm.output_messages.0.message.content": "",
        "llm.output_messages.0.message.function_call_arguments_json": "{}",
        "llm.output_messages.0.message.function_call_name": "getWeather",
        "llm.output_messages.0.message.role": "assistant",
        "openinference.span.kind": "llm",
        "output.mime_type": "text/plain",
        "output.value": "",
      }
  `);
    }));
    it("should not emit a span if tracing is suppressed", () => __awaiter(void 0, void 0, void 0, function* () {
        const response = {
            id: "chatcmpl-8adq9JloOzNZ9TyuzrKyLpGXexh6p",
            object: "chat.completion",
            created: 1703743645,
            model: "gpt-3.5-turbo-0613",
            choices: [
                {
                    index: 0,
                    message: {
                        role: "assistant",
                        content: "This is a test.",
                    },
                    logprobs: null,
                    finish_reason: "stop",
                },
            ],
            usage: {
                prompt_tokens: 12,
                completion_tokens: 5,
                total_tokens: 17,
            },
        };
        // Mock out the chat completions endpoint
        jest.spyOn(openai, "post").mockImplementation(
        // @ts-expect-error the response type is not correct - this is just for testing
        () => __awaiter(void 0, void 0, void 0, function* () {
            return response;
        }));
        const _response = yield new Promise((resolve, _reject) => {
            api_1.context.with((0, core_1.suppressTracing)(api_1.context.active()), () => {
                resolve(openai.chat.completions.create({
                    messages: [{ role: "user", content: "Say this is a test" }],
                    model: "gpt-3.5-turbo",
                }));
            });
        });
        const spans = memoryExporter.getFinishedSpans();
        expect(spans.length).toBe(0);
    }));
});
//# sourceMappingURL=openai.test.js.map
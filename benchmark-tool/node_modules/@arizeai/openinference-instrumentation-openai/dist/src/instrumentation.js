"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __rest = (this && this.__rest) || function (s, e) {
    var t = {};
    for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p) && e.indexOf(p) < 0)
        t[p] = s[p];
    if (s != null && typeof Object.getOwnPropertySymbols === "function")
        for (var i = 0, p = Object.getOwnPropertySymbols(s); i < p.length; i++) {
            if (e.indexOf(p[i]) < 0 && Object.prototype.propertyIsEnumerable.call(s, p[i]))
                t[p[i]] = s[p[i]];
        }
    return t;
};
var __asyncValues = (this && this.__asyncValues) || function (o) {
    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");
    var m = o[Symbol.asyncIterator], i;
    return m ? m.call(o) : (o = typeof __values === "function" ? __values(o) : o[Symbol.iterator](), i = {}, verb("next"), verb("throw"), verb("return"), i[Symbol.asyncIterator] = function () { return this; }, i);
    function verb(n) { i[n] = o[n] && function (v) { return new Promise(function (resolve, reject) { v = o[n](v), settle(resolve, reject, v.done, v.value); }); }; }
    function settle(resolve, reject, d, v) { Promise.resolve(v).then(function(v) { resolve({ value: v, done: d }); }, reject); }
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.OpenAIInstrumentation = exports.isPatched = void 0;
const instrumentation_1 = require("@opentelemetry/instrumentation");
const api_1 = require("@opentelemetry/api");
const version_1 = require("./version");
const openinference_semantic_conventions_1 = require("@arizeai/openinference-semantic-conventions");
const typeUtils_1 = require("./typeUtils");
const core_1 = require("@opentelemetry/core");
const MODULE_NAME = "openai";
/**
 * Flag to check if the openai module has been patched
 * Note: This is a fallback in case the module is made immutable (e.x. Deno, webpack, etc.)
 */
let _isOpenInferencePatched = false;
/**
 * function to check if instrumentation is enabled / disabled
 */
function isPatched() {
    return _isOpenInferencePatched;
}
exports.isPatched = isPatched;
/**
 * Resolves the execution context for the current span
 * If tracing is suppressed, the span is dropped and the current context is returned
 * @param span
 */
function getExecContext(span) {
    const activeContext = api_1.context.active();
    const suppressTracing = (0, core_1.isTracingSuppressed)(activeContext);
    const execContext = suppressTracing
        ? api_1.trace.setSpan(api_1.context.active(), span)
        : activeContext;
    // Drop the span from the context
    if (suppressTracing) {
        api_1.trace.deleteSpan(activeContext);
    }
    return execContext;
}
class OpenAIInstrumentation extends instrumentation_1.InstrumentationBase {
    constructor(config) {
        super("@arizeai/openinference-instrumentation-openai", version_1.VERSION, Object.assign({}, config));
    }
    init() {
        const module = new instrumentation_1.InstrumentationNodeModuleDefinition("openai", ["^4.0.0"], this.patch.bind(this), this.unpatch.bind(this));
        return module;
    }
    /**
     * Manually instruments the OpenAI module. This is needed when the module is not loaded via require (commonjs)
     * @param {openai} module
     */
    manuallyInstrument(module) {
        api_1.diag.debug(`Manually instrumenting ${MODULE_NAME}`);
        this.patch(module);
    }
    /**
     * Patches the OpenAI module
     */
    patch(module, moduleVersion) {
        api_1.diag.debug(`Applying patch for ${MODULE_NAME}@${moduleVersion}`);
        if ((module === null || module === void 0 ? void 0 : module.openInferencePatched) || _isOpenInferencePatched) {
            return module;
        }
        // eslint-disable-next-line @typescript-eslint/no-this-alias
        const instrumentation = this;
        // Patch create chat completions
        this._wrap(module.OpenAI.Chat.Completions.prototype, "create", 
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        (original) => {
            return function patchedCreate(...args) {
                const body = args[0];
                const { messages: _messages } = body, invocationParameters = __rest(body, ["messages"]);
                const span = instrumentation.tracer.startSpan(`OpenAI Chat Completions`, {
                    kind: api_1.SpanKind.INTERNAL,
                    attributes: Object.assign({ [openinference_semantic_conventions_1.SemanticConventions.OPENINFERENCE_SPAN_KIND]: openinference_semantic_conventions_1.OpenInferenceSpanKind.LLM, [openinference_semantic_conventions_1.SemanticConventions.LLM_MODEL_NAME]: body.model, [openinference_semantic_conventions_1.SemanticConventions.INPUT_VALUE]: JSON.stringify(body), [openinference_semantic_conventions_1.SemanticConventions.INPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.JSON, [openinference_semantic_conventions_1.SemanticConventions.LLM_INVOCATION_PARAMETERS]: JSON.stringify(invocationParameters) }, getLLMInputMessagesAttributes(body)),
                });
                const execContext = getExecContext(span);
                const execPromise = (0, instrumentation_1.safeExecuteInTheMiddle)(() => {
                    return api_1.context.with(execContext, () => {
                        return original.apply(this, args);
                    });
                }, (error) => {
                    // Push the error to the span
                    if (error) {
                        span.recordException(error);
                        span.setStatus({
                            code: api_1.SpanStatusCode.ERROR,
                            message: error.message,
                        });
                        span.end();
                    }
                });
                const wrappedPromise = execPromise.then((result) => {
                    if (isChatCompletionResponse(result)) {
                        // Record the results
                        span.setAttributes(Object.assign(Object.assign({ [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_VALUE]: JSON.stringify(result), [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.JSON, 
                            // Override the model from the value sent by the server
                            [openinference_semantic_conventions_1.SemanticConventions.LLM_MODEL_NAME]: result.model }, getChatCompletionLLMOutputMessagesAttributes(result)), getUsageAttributes(result)));
                        span.setStatus({ code: api_1.SpanStatusCode.OK });
                        span.end();
                    }
                    else {
                        // This is a streaming response
                        // handle the chunks and add them to the span
                        // First split the stream via tee
                        const [leftStream, rightStream] = result.tee();
                        consumeChatCompletionStreamChunks(rightStream, span);
                        result = leftStream;
                    }
                    return result;
                });
                return api_1.context.bind(execContext, wrappedPromise);
            };
        });
        this._wrap(module.OpenAI.Completions.prototype, "create", 
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        (original) => {
            return function patchedCreate(...args) {
                const body = args[0];
                const { prompt: _prompt } = body, invocationParameters = __rest(body, ["prompt"]);
                const span = instrumentation.tracer.startSpan(`OpenAI Completions`, {
                    kind: api_1.SpanKind.INTERNAL,
                    attributes: Object.assign({ [openinference_semantic_conventions_1.SemanticConventions.OPENINFERENCE_SPAN_KIND]: openinference_semantic_conventions_1.OpenInferenceSpanKind.LLM, [openinference_semantic_conventions_1.SemanticConventions.LLM_MODEL_NAME]: body.model, [openinference_semantic_conventions_1.SemanticConventions.LLM_INVOCATION_PARAMETERS]: JSON.stringify(invocationParameters) }, getCompletionInputValueAndMimeType(body)),
                });
                const execContext = getExecContext(span);
                const execPromise = (0, instrumentation_1.safeExecuteInTheMiddle)(() => {
                    return api_1.context.with(execContext, () => {
                        return original.apply(this, args);
                    });
                }, (error) => {
                    // Push the error to the span
                    if (error) {
                        span.recordException(error);
                        span.setStatus({
                            code: api_1.SpanStatusCode.ERROR,
                            message: error.message,
                        });
                        span.end();
                    }
                });
                const wrappedPromise = execPromise.then((result) => {
                    if (isCompletionResponse(result)) {
                        // Record the results
                        span.setAttributes(Object.assign(Object.assign({ [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_VALUE]: JSON.stringify(result), [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.JSON, 
                            // Override the model from the value sent by the server
                            [openinference_semantic_conventions_1.SemanticConventions.LLM_MODEL_NAME]: result.model }, getCompletionOutputValueAndMimeType(result)), getUsageAttributes(result)));
                        span.setStatus({ code: api_1.SpanStatusCode.OK });
                        span.end();
                    }
                    return result;
                });
                return api_1.context.bind(execContext, wrappedPromise);
            };
        });
        this._wrap(module.OpenAI.Embeddings.prototype, "create", 
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        (original) => {
            return function patchedEmbeddingCreate(...args) {
                const body = args[0];
                const { input } = body;
                const isStringInput = typeof input === "string";
                const span = instrumentation.tracer.startSpan(`OpenAI Embeddings`, {
                    kind: api_1.SpanKind.INTERNAL,
                    attributes: Object.assign({ [openinference_semantic_conventions_1.SemanticConventions.OPENINFERENCE_SPAN_KIND]: openinference_semantic_conventions_1.OpenInferenceSpanKind.EMBEDDING, [openinference_semantic_conventions_1.SemanticConventions.EMBEDDING_MODEL_NAME]: body.model, [openinference_semantic_conventions_1.SemanticConventions.INPUT_VALUE]: isStringInput
                            ? input
                            : JSON.stringify(input), [openinference_semantic_conventions_1.SemanticConventions.INPUT_MIME_TYPE]: isStringInput
                            ? openinference_semantic_conventions_1.MimeType.TEXT
                            : openinference_semantic_conventions_1.MimeType.JSON }, getEmbeddingTextAttributes(body)),
                });
                const execContext = getExecContext(span);
                const execPromise = (0, instrumentation_1.safeExecuteInTheMiddle)(() => {
                    return api_1.context.with(execContext, () => {
                        return original.apply(this, args);
                    });
                }, (error) => {
                    // Push the error to the span
                    if (error) {
                        span.recordException(error);
                        span.setStatus({
                            code: api_1.SpanStatusCode.ERROR,
                            message: error.message,
                        });
                        span.end();
                    }
                });
                const wrappedPromise = execPromise.then((result) => {
                    if (result) {
                        // Record the results
                        span.setAttributes(Object.assign({}, getEmbeddingEmbeddingsAttributes(result)));
                    }
                    span.setStatus({ code: api_1.SpanStatusCode.OK });
                    span.end();
                    return result;
                });
                return api_1.context.bind(execContext, wrappedPromise);
            };
        });
        _isOpenInferencePatched = true;
        try {
            // This can fail if the module is made immutable via the runtime or bundler
            module.openInferencePatched = true;
        }
        catch (e) {
            api_1.diag.warn(`Failed to set ${MODULE_NAME} patched flag on the module`, e);
        }
        return module;
    }
    /**
     * Un-patches the OpenAI module's chat completions API
     */
    unpatch(moduleExports, moduleVersion) {
        api_1.diag.debug(`Removing patch for ${MODULE_NAME}@${moduleVersion}`);
        this._unwrap(moduleExports.OpenAI.Chat.Completions.prototype, "create");
        this._unwrap(moduleExports.OpenAI.Completions.prototype, "create");
        this._unwrap(moduleExports.OpenAI.Embeddings.prototype, "create");
        _isOpenInferencePatched = false;
        try {
            // This can fail if the module is made immutable via the runtime or bundler
            moduleExports.openInferencePatched = false;
        }
        catch (e) {
            api_1.diag.warn(`Failed to unset ${MODULE_NAME} patched flag on the module`, e);
        }
    }
}
exports.OpenAIInstrumentation = OpenAIInstrumentation;
/**
 * type-guard that checks if the response is a chat completion response
 */
function isChatCompletionResponse(response) {
    return "choices" in response;
}
/**
 * type-guard that checks if the response is a completion response
 */
function isCompletionResponse(response) {
    return "choices" in response;
}
/**
 * type-guard that checks if completion prompt attribute is an array of strings
 */
function isPromptStringArray(prompt) {
    return (Array.isArray(prompt) && prompt.every((item) => typeof item === "string"));
}
/**
 * Converts the body of a chat completions request to LLM input messages
 */
function getLLMInputMessagesAttributes(body) {
    return body.messages.reduce((acc, message, index) => {
        const messageAttributes = getChatCompletionInputMessageAttributes(message);
        const indexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.LLM_INPUT_MESSAGES}.${index}.`;
        // Flatten the attributes on the index prefix
        for (const [key, value] of Object.entries(messageAttributes)) {
            acc[`${indexPrefix}${key}`] = value;
        }
        return acc;
    }, {});
}
function getChatCompletionInputMessageAttributes(message) {
    const role = message.role;
    const attributes = {
        [openinference_semantic_conventions_1.SemanticConventions.MESSAGE_ROLE]: role,
    };
    // Add the content only if it is a string
    if (typeof message.content === "string")
        attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT] = message.content;
    switch (role) {
        case "user":
            // There's nothing to add for the user
            break;
        case "assistant":
            if (message.tool_calls) {
                message.tool_calls.forEach((toolCall, index) => {
                    // Make sure the tool call has a function
                    if (toolCall.function) {
                        const toolCallIndexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_TOOL_CALLS}.${index}.`;
                        attributes[toolCallIndexPrefix + openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_NAME] = toolCall.function.name;
                        attributes[toolCallIndexPrefix +
                            openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_ARGUMENTS_JSON] = toolCall.function.arguments;
                    }
                });
            }
            break;
        case "function":
            attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_FUNCTION_CALL_NAME] = message.name;
            break;
        case "tool":
            // There's nothing to add for the tool. There is a tool_id, but there are no
            // semantic conventions for it
            break;
        case "system":
            // There's nothing to add for the system. Content is captured above
            break;
        default:
            (0, typeUtils_1.assertUnreachable)(role);
            break;
    }
    return attributes;
}
/**
 * Converts the body of a completions request to input attributes
 */
function getCompletionInputValueAndMimeType(body) {
    if (typeof body.prompt === "string") {
        return {
            [openinference_semantic_conventions_1.SemanticConventions.INPUT_VALUE]: body.prompt,
            [openinference_semantic_conventions_1.SemanticConventions.INPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.TEXT,
        };
    }
    else if (isPromptStringArray(body.prompt)) {
        const prompt = body.prompt[0]; // Only single prompts are currently supported
        if (prompt === undefined) {
            return {};
        }
        return {
            [openinference_semantic_conventions_1.SemanticConventions.INPUT_VALUE]: prompt,
            [openinference_semantic_conventions_1.SemanticConventions.INPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.TEXT,
        };
    }
    // Other cases in which the prompt is a token or array of tokens are currently unsupported
    return {};
}
/**
 * Get usage attributes
 */
function getUsageAttributes(completion) {
    if (completion.usage) {
        return {
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_COMPLETION]: completion.usage.completion_tokens,
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_PROMPT]: completion.usage.prompt_tokens,
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_TOTAL]: completion.usage.total_tokens,
        };
    }
    return {};
}
/**
 * Converts the chat completion result to LLM output attributes
 */
function getChatCompletionLLMOutputMessagesAttributes(chatCompletion) {
    // Right now support just the first choice
    const choice = chatCompletion.choices[0];
    if (!choice) {
        return {};
    }
    return [choice.message].reduce((acc, message, index) => {
        const indexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.LLM_OUTPUT_MESSAGES}.${index}.`;
        const messageAttributes = getChatCompletionOutputMessageAttributes(message);
        // Flatten the attributes on the index prefix
        for (const [key, value] of Object.entries(messageAttributes)) {
            acc[`${indexPrefix}${key}`] = value;
        }
        return acc;
    }, {});
}
function getChatCompletionOutputMessageAttributes(message) {
    const role = message.role;
    const attributes = {
        [openinference_semantic_conventions_1.SemanticConventions.MESSAGE_ROLE]: role,
    };
    if (message.content) {
        attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT] = message.content;
    }
    if (message.tool_calls) {
        message.tool_calls.forEach((toolCall, index) => {
            const toolCallIndexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_TOOL_CALLS}.${index}.`;
            // Double check that the tool call has a function
            // NB: OpenAI only supports tool calls with functions right now but this may change
            if (toolCall.function) {
                attributes[toolCallIndexPrefix + openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_NAME] = toolCall.function.name;
                attributes[toolCallIndexPrefix +
                    openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_ARGUMENTS_JSON] = toolCall.function.arguments;
            }
        });
    }
    if (message.function_call) {
        attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_FUNCTION_CALL_NAME] =
            message.function_call.name;
        attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_FUNCTION_CALL_ARGUMENTS_JSON] =
            message.function_call.arguments;
    }
    return attributes;
}
/**
 * Converts the completion result to output attributes
 */
function getCompletionOutputValueAndMimeType(completion) {
    // Right now support just the first choice
    const choice = completion.choices[0];
    if (!choice) {
        return {};
    }
    return {
        [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_VALUE]: String(choice.text),
        [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.TEXT,
    };
}
/**
 * Converts the embedding result payload to embedding attributes
 */
function getEmbeddingTextAttributes(request) {
    if (typeof request.input === "string") {
        return {
            [`${openinference_semantic_conventions_1.SemanticConventions.EMBEDDING_EMBEDDINGS}.0.${openinference_semantic_conventions_1.SemanticConventions.EMBEDDING_TEXT}`]: request.input,
        };
    }
    else if (Array.isArray(request.input) &&
        request.input.length > 0 &&
        typeof request.input[0] === "string") {
        return request.input.reduce((acc, input, index) => {
            const indexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.EMBEDDING_EMBEDDINGS}.${index}.`;
            acc[`${indexPrefix}${openinference_semantic_conventions_1.SemanticConventions.EMBEDDING_TEXT}`] = input;
            return acc;
        }, {});
    }
    // Ignore other cases where input is a number or an array of numbers
    return {};
}
/**
 * Converts the embedding result payload to embedding attributes
 */
function getEmbeddingEmbeddingsAttributes(response) {
    return response.data.reduce((acc, embedding, index) => {
        const indexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.EMBEDDING_EMBEDDINGS}.${index}.`;
        acc[`${indexPrefix}${openinference_semantic_conventions_1.SemanticConventions.EMBEDDING_VECTOR}`] =
            embedding.embedding;
        return acc;
    }, {});
}
/**
 * Consumes the stream chunks and adds them to the span
 */
function consumeChatCompletionStreamChunks(stream, span) {
    var _a, stream_1, stream_1_1;
    var _b, e_1, _c, _d;
    return __awaiter(this, void 0, void 0, function* () {
        let streamResponse = "";
        // Tool and function call attributes can also arrive in the stream
        // NB: the tools and function calls arrive in partial diffs
        // So the final tool and function calls need to be aggregated
        // across chunks
        const toolAndFunctionCallAttributes = {};
        try {
            // The first message is for the assistant response so we start at 1
            for (_a = true, stream_1 = __asyncValues(stream); stream_1_1 = yield stream_1.next(), _b = stream_1_1.done, !_b; _a = true) {
                _d = stream_1_1.value;
                _a = false;
                const chunk = _d;
                if (chunk.choices.length <= 0) {
                    continue;
                }
                const choice = chunk.choices[0];
                if (choice.delta.content) {
                    streamResponse += choice.delta.content;
                }
                // Accumulate the tool and function call attributes
                const toolAndFunctionCallAttributesDiff = getToolAndFunctionCallAttributesFromStreamChunk(chunk);
                for (const [key, value] of Object.entries(toolAndFunctionCallAttributesDiff)) {
                    if ((0, typeUtils_1.isString)(toolAndFunctionCallAttributes[key]) && (0, typeUtils_1.isString)(value)) {
                        toolAndFunctionCallAttributes[key] += value;
                    }
                    else if ((0, typeUtils_1.isString)(value)) {
                        toolAndFunctionCallAttributes[key] = value;
                    }
                }
            }
        }
        catch (e_1_1) { e_1 = { error: e_1_1 }; }
        finally {
            try {
                if (!_a && !_b && (_c = stream_1.return)) yield _c.call(stream_1);
            }
            finally { if (e_1) throw e_1.error; }
        }
        const messageIndexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.LLM_OUTPUT_MESSAGES}.0.`;
        // Append the attributes to the span as a message
        const attributes = {
            [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_VALUE]: streamResponse,
            [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.TEXT,
            [`${messageIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT}`]: streamResponse,
            [`${messageIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_ROLE}`]: "assistant",
        };
        // Add the tool and function call attributes
        for (const [key, value] of Object.entries(toolAndFunctionCallAttributes)) {
            attributes[`${messageIndexPrefix}${key}`] = value;
        }
        span.setAttributes(attributes);
        span.end();
    });
}
/**
 * Extracts the semantic attributes from the stream chunk for tool_calls and function_calls
 */
function getToolAndFunctionCallAttributesFromStreamChunk(chunk) {
    if (chunk.choices.length <= 0) {
        return {};
    }
    const choice = chunk.choices[0];
    const attributes = {};
    if (choice.delta.tool_calls) {
        choice.delta.tool_calls.forEach((toolCall, index) => {
            const toolCallIndexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_TOOL_CALLS}.${index}.`;
            // Double check that the tool call has a function
            // NB: OpenAI only supports tool calls with functions right now but this may change
            if (toolCall.function) {
                attributes[toolCallIndexPrefix + openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_NAME] = toolCall.function.name;
                attributes[toolCallIndexPrefix +
                    openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_ARGUMENTS_JSON] = toolCall.function.arguments;
            }
        });
    }
    if (choice.delta.function_call) {
        attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_FUNCTION_CALL_NAME] =
            choice.delta.function_call.name;
        attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_FUNCTION_CALL_ARGUMENTS_JSON] =
            choice.delta.function_call.arguments;
    }
    return attributes;
}
//# sourceMappingURL=instrumentation.js.map